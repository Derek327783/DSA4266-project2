{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e73a0444",
   "metadata": {},
   "source": [
    "# Libraries and Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "940eac23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import gzip\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec82b7e4",
   "metadata": {},
   "source": [
    "# Parse data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a32412f",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './Data/dataset0.json.gz'\n",
    "dataset1 = './Data/dataset1.json.gz'\n",
    "dataset2 = './Data/dataset2.json.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b779a01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_data_list = []\n",
    "dataset1_parsed = []\n",
    "dataset2_parsed = []\n",
    "\n",
    "# Read and decompress the file, parsing each line as a separate JSON object\n",
    "with gzip.open(file_path, 'rt', encoding='utf-8') as gzip_file:\n",
    "    for line in gzip_file:\n",
    "        try:\n",
    "            data = json.loads(line)\n",
    "            # Append the parsed JSON object to the list\n",
    "            for i in data.keys():\n",
    "                for j in data[i].keys():\n",
    "                    for z in data[i][j].keys():                  \n",
    "                        parsed_data_list.append([i, j, z, data[i][j][z]])\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error parsing JSON on line: {line}\")\n",
    "            \n",
    "with gzip.open(dataset1, 'rt', encoding='utf-8') as gzip_file:\n",
    "    for line in gzip_file:\n",
    "        try:\n",
    "            data = json.loads(line)\n",
    "            # Append the parsed JSON object to the list\n",
    "            for i in data.keys():\n",
    "                for j in data[i].keys():\n",
    "                    for z in data[i][j].keys():                  \n",
    "                        dataset1_parsed.append([i, j, z, data[i][j][z]])\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error parsing JSON on line: {line}\")\n",
    "            \n",
    "with gzip.open(dataset2, 'rt', encoding='utf-8') as gzip_file:\n",
    "    for line in gzip_file:\n",
    "        try:\n",
    "            data = json.loads(line)\n",
    "            # Append the parsed JSON object to the list\n",
    "            for i in data.keys():\n",
    "                for j in data[i].keys():\n",
    "                    for z in data[i][j].keys():                  \n",
    "                        dataset2_parsed.append([i, j, z, data[i][j][z]])\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error parsing JSON on line: {line}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86ff1b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(parsed_data_list)\n",
    "dataset1 = pd.DataFrame(dataset1_parsed)\n",
    "dataset2 = pd.DataFrame(dataset2_parsed)\n",
    "\n",
    "data_info = pd.read_csv(\"./Data/data.info\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9332e5c2",
   "metadata": {},
   "source": [
    "# Cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2caa7011",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.rename(columns={0: 'Transcript_ID', 1: 'Position', 2:\"Base_seq\", 3:\"Sample_reads\"})\n",
    "dataset1 = dataset1.rename(columns={0: 'Transcript_ID', 1: 'Position', 2:\"Base_seq\", 3:\"Sample_reads\"})\n",
    "dataset2 = dataset2.rename(columns={0: 'Transcript_ID', 1: 'Position', 2:\"Base_seq\", 3:\"Sample_reads\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44aa6f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean and std and create another col 'Compiled_reads'\n",
    "def feature_engin1(rows):\n",
    "    sample_reads = rows[\"Sample_reads\"]\n",
    "    output = {\"Mean\":[],\"Standard_deviation\":[]}\n",
    "    matrix = np.array(sample_reads)\n",
    "    column_means = np.mean(matrix, axis=0)\n",
    "    column_std = np.std(matrix,axis=0)\n",
    "    output = [list(column_means),list(column_std)]\n",
    "    return output\n",
    "\n",
    "data[\"Compiled_reads\"] = data.apply(feature_engin1,axis=1)\n",
    "dataset1[\"Compiled_reads\"] = dataset1.apply(feature_engin1,axis=1)\n",
    "dataset2[\"Compiled_reads\"] = dataset2.apply(feature_engin1,axis=1)\n",
    "data_info = data_info.rename(columns={'transcript_id': 'Transcript_ID',\"transcript_position\":\"Position\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75339f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Position'] = data['Position'].astype('int64')\n",
    "dataset1['Position'] = dataset1['Position'].astype('int64')\n",
    "dataset2['Position'] = dataset2['Position'].astype('int64')\n",
    "\n",
    "data_info['Position'] = data_info['Position'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aea63a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the data_info and data together\n",
    "merged_right = pd.merge(data_info, data, on=['Transcript_ID',\"Position\"], how='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b59e0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the compiled reads into separate columns\n",
    "new_columns = merged_right['Compiled_reads'].apply(pd.Series).apply(pd.Series)\n",
    "new_columns = pd.concat([new_columns[0].apply(pd.Series), new_columns[1].apply(pd.Series)], axis = 1)\n",
    "new_columns.columns = ['mean1', 'mean2', 'mean3', 'mean4', 'mean5', 'mean6', 'mean7', 'mean8', 'mean9',\n",
    "              'sd1', 'sd2', 'sd3', 'sd4', 'sd5', 'sd6', 'sd7', 'sd8', 'sd9']\n",
    "\n",
    "new_columns1 = dataset1['Compiled_reads'].apply(pd.Series).apply(pd.Series)\n",
    "new_columns1 = pd.concat([new_columns1[0].apply(pd.Series), new_columns1[1].apply(pd.Series)], axis = 1)\n",
    "new_columns1.columns = ['mean1', 'mean2', 'mean3', 'mean4', 'mean5', 'mean6', 'mean7', 'mean8', 'mean9',\n",
    "              'sd1', 'sd2', 'sd3', 'sd4', 'sd5', 'sd6', 'sd7', 'sd8', 'sd9']\n",
    "\n",
    "new_columns2 = dataset2['Compiled_reads'].apply(pd.Series).apply(pd.Series)\n",
    "new_columns2 = pd.concat([new_columns2[0].apply(pd.Series), new_columns2[1].apply(pd.Series)], axis = 1)\n",
    "new_columns2.columns = ['mean1', 'mean2', 'mean3', 'mean4', 'mean5', 'mean6', 'mean7', 'mean8', 'mean9',\n",
    "              'sd1', 'sd2', 'sd3', 'sd4', 'sd5', 'sd6', 'sd7', 'sd8', 'sd9']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0b64abe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_right_processed = pd.concat([merged_right,new_columns],axis = 1)\n",
    "merged_right_processed = merged_right_processed.drop(columns = [\"Sample_reads\",\"Compiled_reads\"])\n",
    "\n",
    "merged_right_processed1 = pd.concat([dataset1,new_columns1],axis = 1)\n",
    "merged_right_processed1 = merged_right_processed1.drop(columns = [\"Sample_reads\",\"Compiled_reads\"])\n",
    "\n",
    "merged_right_processed2 = pd.concat([dataset2,new_columns2],axis = 1)\n",
    "merged_right_processed2 = merged_right_processed2.drop(columns = [\"Sample_reads\",\"Compiled_reads\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7bd7bb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = merged_right_processed\n",
    "test_data1 = merged_right_processed1\n",
    "test_data2 = merged_right_processed2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e06ff1b",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1ed40f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_undersample(train_data):\n",
    "    rus = RandomUnderSampler()\n",
    "    X = train_data.drop(columns = [\"label\",\"gene_id\",\"Transcript_ID\",\"Base_seq\"])\n",
    "    Y = train_data['label'] \n",
    "    X_resampled, y_resampled = rus.fit_resample(X, Y)\n",
    "    output = pd.concat([X_resampled,y_resampled],axis = 1) \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d9dc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_data = random_undersample(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "12de2b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = new_train_data.drop('label', axis=1)\n",
    "Y_train = new_train_data['label']\n",
    "X_test = test_data1.drop(columns = [\"Transcript_ID\",\"Base_seq\"],axis = 1)\n",
    "\n",
    "model = xgb.XGBClassifier(\n",
    "    objective='binary:logistic', \n",
    "    n_estimators=100,            \n",
    "    max_depth=3,                \n",
    "    learning_rate=0.1,          \n",
    "    subsample=0.8,             \n",
    "    colsample_bytree=0.8,       \n",
    ")\n",
    "\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "Y_pred = model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0fd2063a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame({\n",
    "    'transcript_id': test_data1['Transcript_ID'],\n",
    "    'transcript_position': test_data1['Position'],\n",
    "    'score': Y_pred[:, 1]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8ea52e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv('dataset1_score.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a2567a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = new_train_data.drop('label', axis=1)\n",
    "Y_train = new_train_data['label']\n",
    "X_test = test_data2.drop(columns = [\"Transcript_ID\",\"Base_seq\"],axis = 1)\n",
    "\n",
    "model = xgb.XGBClassifier(\n",
    "    objective='binary:logistic', \n",
    "    n_estimators=100,            \n",
    "    max_depth=3,                \n",
    "    learning_rate=0.1,          \n",
    "    subsample=0.8,             \n",
    "    colsample_bytree=0.8,       \n",
    ")\n",
    "\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "Y_pred = model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "130406e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame({\n",
    "    'transcript_id': test_data2['Transcript_ID'],\n",
    "    'transcript_position': test_data2['Position'],\n",
    "    'score': Y_pred[:, 1]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2cdc5cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv('dataset2_score.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6d1a8e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = new_train_data.drop('label', axis=1)\n",
    "Y_train = new_train_data['label']\n",
    "X_test = train_data.drop(columns = [\"gene_id\", \"label\", \"Transcript_ID\",\"Base_seq\"],axis = 1)\n",
    "\n",
    "model = xgb.XGBClassifier(\n",
    "    objective='binary:logistic', \n",
    "    n_estimators=100,            \n",
    "    max_depth=3,                \n",
    "    learning_rate=0.1,          \n",
    "    subsample=0.8,             \n",
    "    colsample_bytree=0.8,       \n",
    ")\n",
    "\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "Y_pred = model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "79ae342b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame({\n",
    "    'transcript_id': train_data['Transcript_ID'],\n",
    "    'transcript_position': train_data['Position'],\n",
    "    'score': Y_pred[:, 1]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8d4c50ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv('dataset0_score.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
